{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SPAM Dataset visualization",
   "id": "82e6e860d9a50ec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from src.preprocessing import data_spam, load_data, preprocessing, visualize",
   "id": "fc7c996a8ed9788b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we charge the dataframe from the spambase : 4600 emails with 58 features",
   "id": "1954f3157d6cfc5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_, _, df, features = load_data(data_spam)\n",
    "df"
   ],
   "id": "bf661f7704c3c6ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And verify with the .info() method.\n",
    "\n",
    "We also check that there is no missing values in each of the columns"
   ],
   "id": "638d9def02eb05ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()",
   "id": "8ebdc7b25783fc35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using the .DOCUMENTATION file, we learn that each feature named \"word_freq_*WORD*\" represent the percentage of word in the email which **are** the word *WORD*. (48 features of this type)\n",
    "\n",
    "For exemple :\n",
    "- **word_freq_credit** gives the percentage of word in the email that match the word \"*credit*\"\n",
    "- **word_freq_report** same with word \"*report*\"\n",
    "\n",
    "6 features are named \"char_freq_*CHAR*\" which is the same as before but with a character *CHAR*.\n",
    "\n",
    "Exemples :\n",
    "- **char_freq_;**\n",
    "- **char_freq_$**\n",
    "\n",
    "The remaining columns are : \n",
    "\n",
    "**capital_run_length_average** = average length of uninterrupted sequences of capital letters\n",
    "\n",
    "**capital_run_length_longest** = length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "**capital_run_length_total** = sum of length of uninterrupted sequences of capital letters = total number of capital letters in the e-mail\n",
    "\n",
    "**is_spam** = denotes whether the e-mail was considered spam (1) or not (0),  i.e. unsolicited commercial e-mail"
   ],
   "id": "c3059a024539577f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe()",
   "id": "8bcd61cec81e9713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We check the number of spam in the dataset :",
   "id": "96fc659b6d590a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[\"Class\"].value_counts()",
   "id": "5ac2a7f942dec231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For some of the features we can see how the data is distributed, distinguishing spam and no spam, and the different correlations\n",
    "Most of the other features have their density concentrated around 0.\n",
    "\n",
    "2D correlation here is not quite relevent because there is a lot of superposition and datas are concentrated.\n",
    "Correlation matrix is way more usefull to understand this."
   ],
   "id": "6140e3dd1c7cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(features)\n",
    "\n",
    "visualize(data_spam, features[:5], random=False)"
   ],
   "id": "f886e5b73126906a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Objectives",
   "id": "2552f1a79c118fde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The objective of this notebook is to build and compare several Machine Learning models on the Spam dataset. We will evaluate four different approaches — K-Nearest Neighbors (KNN), a Multilayer Perceptron Neural Network (MLP), a Random Forest classifier, and a Linear Support Vector Classifier (Linear SVC). For each model, we will train, evaluate, and compare their prediction accuracies. In addition, we will analyze feature importance to better understand which variables contribute the most to the classification performance.",
   "id": "112e48ef134e9297"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Functions and Workflow Description",
   "id": "173f90f24c934cdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The preprocessing function loads the dataset, normalize the continuous features (not the binary-ones), and return a train, val, test split.\n",
    "For each model, we defined it, set a train and a predict function. We train the model with the split we made in preprocessing.\n",
    "Then we call the benchmark function (same for every model), which uses the predict function, and returns an accuracy report, a confusion matrix and the overall feature importance. It allows us to compare models between each other.\n"
   ],
   "id": "713bd756eb37496d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# K nearest neighbour\n",
   "id": "9f2c955fda9b00af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.knn import KNNModel\n",
    "\n",
    "samples = preprocessing(data=data_spam, test_size=0.3, validation_size=0.1)\n",
    "model = KNNModel()\n",
    "model.train(x=samples.X_train, y=samples.y_train)\n",
    "model.benchmark(x=samples.X_test, y=samples.y_test)"
   ],
   "id": "c6ac1864e57cefc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91fe50996883b1ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysis\n",
   "id": "4f9ab76bf0b05c84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset being a bit unbalanced, what interest us, more than the overall accuracy is the recall and precision, and the f1-score which combine those two metrics. So our objective is to find a compromise between detecting as many spam messages as possible (high recall) while avoiding misclassifying legitimate emails as spam (high precision), which is exactly what the F1-score helps us evaluate.\n",
    "\n",
    "As we can see here with the classification report, a quite simple model such as KNN (K=3) as very good performances. Considering the spam problem, our goal can be not to miss any spam e-mails, so maximize the recall for class 1. Here this is the metric with the less percentage (83,5%) compare to non-spam recall.\n",
    "\n",
    "Next we would like to equilibrate these two recall and maximize them, without influencing the overall precision."
   ],
   "id": "491b2e90eb8ad1ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's do feature selection with the 6 most important features (more than 0.04%) to see what happens",
   "id": "f19874480b902594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "samples = preprocessing(data=data_spam, test_size=0.15, validation_size=0.15)\n",
    "\n",
    "cols = [4, 6, 15, 22, 44, 51]\n",
    "samples.X_train = samples.X_train[:, cols]\n",
    "samples.X_test = samples.X_test[:, cols]\n",
    "samples.X_validation = samples.X_validation[:, cols]\n",
    "\n",
    "model = KNNModel()\n",
    "model.train(x=samples.X_train, y=samples.y_train)\n",
    "model.benchmark(x=samples.X_test, y=samples.y_test)"
   ],
   "id": "2074cf326ffcc4d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There isn't any improvement in the model but the training and prediction is much faster.",
   "id": "3991cb57df3c0f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.nn_interface import MLPModel\n",
    "\n",
    "samples = preprocessing(data=data_spam, test_size=0.15, validation_size=0.15)\n",
    "model = MLPModel(input_size=57, epochs=30)\n",
    "model.train(samples)"
   ],
   "id": "6050c2b5550c887d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.benchmark(samples.X_test, samples.y_test)",
   "id": "4eeccef51124c478"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis",
   "id": "cc5b1e9e0a5944b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we implement a simple Neural Network—an MLP with three hidden layers and 10 units per layer. The model is trained for a total of 30 epochs. We observed that beyond 25 epochs, the validation loss barely improves, which makes this training duration a reasonable choice.\n",
    "\n",
    "Compared to KNN, the results are significantly better: the precision for the positive (spam) class remains around 87% (–2 pts), but the recall increases to 96% (+13.5 pts). This means that the model successfully identifies 96% of spam emails, which is a very strong result. Even though the classifier may incorrectly label some legitimate emails as spam, this type of error has limited consequences.\n",
    "\n",
    "A key advantage is that we improved the most critical metric (recall) without severely compromising the others. Additionally, we can tune the decision threshold applied to the MLP’s probabilistic output. Since the model outputs a probability between 0 and 1, we arbitrarily classify values above 0.5 as spam and below 0.5 as non-spam. Adjusting this threshold allows us to control the trade-off between precision and recall depending on the application’s requirements.\n",
    "\n",
    "We also noticed that the accuracy can vary by up to 2% across training runs, likely due to differences in the model’s random initialization. A potential improvement would be to optimize or stabilize this initialization step in order to achieve more consistent performance."
   ],
   "id": "1219fbf12f221e90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's do feature selection with the 6 most important features (more than 0.04%) to see what happens",
   "id": "bf17ec09103335cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "samples = preprocessing(data=data_spam, test_size=0.15, validation_size=0.15)\n",
    "\n",
    "cols = [6, 24, 26, 45, 52, 55]\n",
    "samples.X_train = samples.X_train[:, cols]\n",
    "samples.X_test = samples.X_test[:, cols]\n",
    "samples.X_validation = samples.X_validation[:, cols]\n",
    "\n",
    "\n",
    "model = MLPModel(input_size=6, epochs=100)\n",
    "model.train(samples)"
   ],
   "id": "4028d004067f2814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.benchmark(samples.X_test, samples.y_test)",
   "id": "992d86ab3dd500ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using only 6 features for spam classification reduces the model’s performance.\n",
    "With such a limited number of features, the model has less information to separate spam from non-spam, which makes learning harder. Consequently, the loss decreases more slowly during training, requiring more epochs, and even then, it does not reach the same minimum loss as when using the full set of features.\n",
    "\n",
    "We decide not to do feature selection anymore as the performance are quite good for this dataset."
   ],
   "id": "61beae1eb7e4b6e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Random Forest",
   "id": "b608759aa297acf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.RForest import RForest\n",
    "\n",
    "samples = preprocessing(data=data_spam, test_size=0.3, validation_size=0.1)\n",
    "model = RForest()\n",
    "model.train(x=samples.X_train, y=samples.y_train)"
   ],
   "id": "bdb04d3c92b0e75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.benchmark(x=samples.X_test, y=samples.y_test)",
   "id": "99bb320923630d0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis",
   "id": "7414892c73303207"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Surprisingly, the Random Forest achieves the best overall performance among all the models we tested. However, if we focus specifically on correctly identifying spam messages, it remains slightly less effective than the MLP (–4 points in recall).\n",
    "\n",
    "Another surprising observation is that the Random Forest relies heavily on the 7th feature (word_freq_remove). While this feature was also important for the other models, it becomes dominant here, accounting for nearly 30% of the total feature importance—far more than in the other classifiers.\n",
    "\n",
    "A plausible explanation for the strong performance of the Random Forest is that decision-tree–based models excel at capturing nonlinear interactions and threshold-based patterns in the data. The spam dataset contains many frequency-based features that behave in a piecewise manner (e.g., presence or absence of certain keywords, sudden increases in word frequency). Random Forests are particularly good at exploiting such structures, allowing them to build diverse trees that capture different aspects of the data. Additionally, their ensemble nature reduces overfitting while improving robustness, which likely contributes to their high overall accuracy."
   ],
   "id": "34daea0e272d07c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Linear SVC",
   "id": "12328bd8e6c0d096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.kernel_methods import LinearSVC_\n",
    "\n",
    "samples = preprocessing(data=data_spam, test_size=0.3, validation_size=0.1)\n",
    "model = LinearSVC_()\n",
    "model.train(x=samples.X_train, y=samples.y_train)"
   ],
   "id": "1fca680ecefb4f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.benchmark(x=samples.X_test, y=samples.y_test)",
   "id": "fced91c375d54bd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "910c1837ac400e4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the spam classification problem, the Random Forest achieves the best overall performance, while the MLP slightly outperforms it in detecting spam emails (class 1). The models generally rely on specific key features, such as word_freq_remove, which strongly influence the predictions. Overall, both the Random Forest and the MLP are suitable choices. The optimal model ultimately depends on the objective: whether we want to catch all spam messages, even if it means misclassifying some legitimate emails (e.g., for children, phishing, etc.), or whether we prefer to maximize spam detection while keeping legitimate emails untouched to avoid missing important messages. Additionally, we can further adjust the decision threshold to fine-tune the trade-off between precision and recall for spam detection.",
   "id": "be62b430f148662e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6564e806c2d0c686"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
